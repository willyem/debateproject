{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'names_and_win' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-af96b63e8b37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames_and_win\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mfor_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maga_\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'debate_text/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#/document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'names_and_win' is not defined"
     ]
    }
   ],
   "source": [
    "import processing\n",
    "import pandas as pd\n",
    "import string \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "names_and_win = [('campus_assault.txt', [0, 1]), ('ISIS_defeated.txt', [0, 1]), ('samesex.txt', [0, 1]), ('iran_deal.txt', [0, 1]),\\\n",
    "('death_penalty.txt', [0, 1]), ('constitutional_authority.txt', [0, 1]), ('right_to_forget.txt', [0, 1]), \\\n",
    "('liberals_stifling.txt', [1, 0]), ('declinists.txt', [1, 0]), ('amazon.txt', [0, 1]), ('gmos.txt', [1, 0]), ('eutha.txt', [0, 1]), \\\n",
    "('income_inequal.txt', [0, 1]), ('mass_collection.txt', [1, 0]), ('flexing_musc.txt', [1, 0]), ('common_core.txt', [1, 0]), \\\n",
    "('pacs.txt', [0, 1]), ('death_not_final.txt', [0, 1]), ('millenials.txt', [1, 0]), ('lecture_obsolete.txt', [1, 0]), \\\n",
    "('russia.txt', [0, 1]), ('USA_kill.txt', [1, 0]), ('affirmative.txt', [1, 0]), ('snowden.txt', [1, 0]), ('obamacare.txt', [1, 0]), \\\n",
    "('eat_face.txt', [1, 0]), ('spy_on_me.txt', [0, 1]), ('right_to_bear.txt', [1, 0]), ('take_job_anywhere.txt', [0, 1]), \\\n",
    "('red_state.txt', [1, 0]), ('break_up_banks.txt', [0, 1]), ('drones.txt', [0, 1]), ('us_syria.txt', [1, 0]), \\\n",
    "('pentagon_budget.txt', [0, 1]), ('fda_caution.txt', [1, 0]), ('gop_center.txt', [0, 1]), ('minimum_wage.txt', [0, 1]), \\\n",
    "('strong_dollar.txt', [1, 0]), ('prohibit_genetic.txt', [0, 1]), ('nuclear_iran.txt', [0, 1]), ('science_god.txt', [1, 0]), \\\n",
    "('legalize_drugs.txt', [1, 0]), ('rich_taxed.txt', [0, 1]), ('end_of_life_care.txt', [1, 0]), ('elected_islamists.txt', [0, 1]),\\\n",
    "('money_politics_overregulated.txt', [0, 1]), ('natural_gas_bad.txt', [1, 0]), ('ban_football.txt', [1, 0]), \\\n",
    "('internet_closing_minds.txt', [1, 0]), ('china_capitalism_better.txt', [0, 1]), ('obesity_govt_business.txt', [0, 1]), \\\n",
    "('palestine_statehood.txt', [1, 0]), ('no_religion.txt', [1, 0]), ('job_plan.txt', [1, 0]), ('too_many_kids_college.txt', [1, 0]), \\\n",
    "('grandmas_benefits.txt', [0, 1]), ('men_are_finished.txt', [1, 0]), ('end_war_on_terror.txt', [0, 1]), \\\n",
    "('freedom_press_state.txt', [0, 1]), ('dont_give_us.txt', [1, 0]), ('clip_americas_wings.txt', [1, 0]), \\\n",
    "('clean_energy.txt', [0, 1]), ('two_party_bad.txt', [0, 1]), ('repeal_obamacare.txt', [0, 1]), ('airports_profiling.txt', [1, 0]), \\\n",
    "('afghanistan_lost.txt', [0, 1]), ('big_govt_stifling.txt', [1, 0]), ('islam_is_peace.txt', [0, 1]), \\\n",
    "('terrorists_enemy_combatants.txt', [0, 1]), ('cyber_war_exaggerated.txt', [0, 1]), ('obamas_policy_us_decline.txt', [0, 1]), \\\n",
    "('organic_is_hype.txt', [0, 1]), ('teacher_unions_failing_schools.txt', [0, 1]), ('us_stepback_israel.txt', [1, 0]), \\\n",
    "('california_failed.txt', [1, 0]), ('us_mexico_drugs.txt', [1, 0]), ('obamas_policies_working.txt', [1, 0]), \\\n",
    "('good_riddance_mainstream.txt', [0, 1]), ('us_will_not_succeed_afghan.txt', [0, 1]), ('buy_american_bad.txt', [1, 0]), \\\n",
    "('diplomacy_iran_nowhere.txt', [0, 1]), ('pay_for_sex.txt', [1, 0]), ('blame_washington_financial.txt', [1, 0]), \\\n",
    "('art_market_ethical.txt', [1, 0]), ('carbon_reductions_not_worth_it.txt', [1, 0]), ('bush_is_worst.txt', [0, 1]), \\\n",
    "('google_dont_be_evil.txt', [1, 0]), ('guns_reduce_crime.txt', [1, 0]), ('america_winning_iraq.txt', [1, 0]), \\\n",
    "('universal_health.txt', [0, 1]), ('legalize_organs.txt', [1, 0]), ('islam_radicals.txt', [1, 0]), \\\n",
    "('tough_interrogation.txt', [0, 1]), ('america_policeman.txt', [1, 0]), ('performance_enhancing.txt', [1, 0]), \\\n",
    "('aid_africa_bad.txt', [0, 1]), ('end_affirmative.txt', [0, 1]), ('russia_enemy_again.txt', [0, 1]), \\\n",
    "('stop_welcoming_immigrants.txt', [1, 0]), ('spread_democracy_me.txt', [1, 0]), ('booming_china.txt', [0, 1]), \\\n",
    "('more_domestic_surveillance.txt', [0, 1]), ('global_warming_not.txt', [1, 0]), ('america_too_religious.txt', [1, 0]), \\\n",
    "('hollywood_anti_us.txt', [0, 1]), ('democratic_hamas.txt', [0, 1]), ('license_to_offend.txt', [1, 0]), \\\n",
    "('tolerate_iran.txt', [1, 0])]\n",
    "'''\n",
    "\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "all_words = [] \n",
    "\n",
    "for document in names_and_win:\n",
    "    for_, aga_=  processing.parse_text('debate_text/'+document[0]) #/document\n",
    "\n",
    "    print for_\n",
    "    print '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
    "    print aga_\n",
    "    print '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
    "    ##tokenize 'for_'\n",
    "    tokenized_for = word_tokenize(for_[0].decode('unicode_escape').encode('ascii', 'ignore'))\n",
    "    tokenized_aga = word_tokenize(aga_[0].decode('unicode_escape').encode('ascii', 'ignore'))\n",
    "\n",
    "    for_wo_punctuation_words = [x for x in tokenized_for if x not in string.punctuation]\n",
    "    aga_wo_punctuation_words = [x for x in tokenized_aga if x not in string.punctuation]\n",
    "\n",
    "    docs_for = [word for word in for_wo_punctuation_words if word not in stop] \n",
    "    docs_aga = [word for word in aga_wo_punctuation_words if word not in stop] \n",
    "\n",
    "    all_words += for_ + aga_\n",
    "\n",
    "#docs_wordnet = [wordnet.lemmatize(word) for word in all_words]\n",
    "\n",
    "#docs = docs_wordnet\n",
    "\n",
    "#vocab_set = set()\n",
    "\n",
    "#[vocab_set.add(token) for token in docs]\n",
    "\n",
    "#vocab = list(vocab_set)\n",
    "\n",
    "#cv = CountVectorizer(stop_words = 'english')\n",
    "#vectorized = cv.fit_transform(for_+aga_)\n",
    "def tokenize(doc):\n",
    "    '''\n",
    "    INPUT: string\n",
    "    OUTPUT: list of strings\n",
    "\n",
    "    Tokenize and stem/lemmatize the document.\n",
    "    '''\n",
    "    return [wordnet.lemmatize(word) for word in word_tokenize(doc.lower())]\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidfed = tfidf.fit_transform(all_words).toarray()\n",
    "\n",
    "#print tfidfed\n",
    "\n",
    "cosine_similarities = linear_kernel(tfidfed, tfidfed)\n",
    "\n",
    "#df = pd.DataFrame(tfidfed)\n",
    "\n",
    "#print cosine_similarities\n",
    "#for index, vector in enumerate(tfidfed): \n",
    "#    if sum(vector) == 0.0:\n",
    "#        print index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
